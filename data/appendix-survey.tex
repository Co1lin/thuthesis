% !TeX root = ../thuthesis-example.tex

\begin{survey}
\label{cha:survey}

\title{Background and Related Works of Fuzzing Deep Learning Libraries}
\maketitle

\tableofcontents

Fuzz testing, or fuzzing, was proposed by Prof. Barton Miller in an operating system course project.\cite{fuzz_unix} They generated random inputs, fed them into several Unix utilities, and saw if there was anything unexpected, such as crashes or wrong outputs. Based on it, people have developed various fuzzing techniques sharing the similar underlying idea to test many software programs, including browsers\cite{corbfuzz}, graphics libraries\cite{fuzz_graphics}, and vehicle systems\cite{fuzz_vehicle}, to strengthen the reliability of software systems.

As deep learning has been becoming more and more popular in recent years, researchers have realized the importance of the reliability of deep learning libraries\cite{emp_tf}, such as PyTorch and TensorFlow, as they are deployed in a variety of applications, and some of them, like self-driving vehicles, are safety-critical scenarios. Therefore, several fuzzing-based testing approaches\cite{predoo, freefuzz, deeprel, cradle, lemon, graphfuzzer, muffin, nnsmith} have been proposed to discover bugs and vulnerabilities hidden inside deep learning libraries, which could be categorized as follows.

\section{API-level Fuzzing}

APIs provided by deep learning libraries are the interface that developers interact with to implement deep learning algorithms. Among thousands of these APIs, tensor operation APIs, also tensor operators (e.g. \texttt{torch.add}, \texttt{torch.nn.Linear}), make up the majority, compared with other ones like \texttt{torch.save} and \texttt{torch.optim.Adam}. Tensor operators are the building blocks of deep learning models, thus it is critical to ensure that they are operating as expected. Either crashes or semantic errors could lead to unpleasant consequences when running models, like giving no results or producing wrong results without any warning. Typical works include Predoo\cite{predoo}, FreeFuzz\cite{freefuzz} and DeepREL\cite{deeprel}.

Freefuzz\cite{freefuzz} is the first general-purpose and fully automated API-level fuzzing approach for testing popular deep learning libraries, which turns out to be very effective. The whole system can be split into four parts:

\begin{itemize}
    \item \textbf{Code collection.} In order to avoid manual construction of API usage to generate test cases, it collects real-world API usage from unit tests by deep learning developers and open-source projects on GitHub.
    \item \textbf{Instrumentation.} Code in both unit tests and open-source projects cannot be directly used as inputs for testing deep learning libraries, as it only needs a single API invocation each time, and other code needs to be pruned. Thanks to the dynamic typing property of Python, attracting API invocations with all input and output data can be easily achieved by instrumenting a list of deep learning APIs at runtime. In detail, each API can be substituted by a wrapper that will record input/output data in a database so that each invocation can be replayed.
    \item \textbf{Mutation.} Replaying collected API invocations can find some bugs for deep learning compilers, since running successfully in eager mode cannot guarantee success in compiled graph mode. However, to fully utilize existing API usages, mutation is commonly applied in fuzzers to generate highly possible valid test cases with minimum effort. In the scenario of testing deep learning APIs, a variety of attributes may be mutated, such as axis/dimension, tensor shapes, or working modes (e.g. padding modes).
    \item \textbf{Differential Testing.} Exceptions and crashes indicate potential bugs. But for semantic errors like wrong output values, ground truth values are needed to indicate bugs, and values given by eager running mode are always regarded as truth in differential testing as it usually involves nothing related to compilation and optimization.
\end{itemize}

Freefuzz is evaluated in two aspects. 1) Coverage at different levels reflects how much code the testing tool touches. At API-level, Freefuzz tested 470 tensor operation APIs for PyTorch and 688 for TensorFlow. 2) The number of discovered bugs indicates the real-world effectiveness of the testing technique. Freefuzz found 28 bugs for PyTorch and 21 for TensorFlow.

\section{Model-level Fuzzing}

Models are the core of deep learning algorithms, like ResNet\cite{resnet} for computer vision and Transformers\cite{attention} for natural language processing. Models consist of well-connected tensor operators, but improving operators' reliability with API-level fuzzing is not enough to build reliable deep learning systems. It is because of the existence of deep learning compilers, like TVM\cite{tvm}, XLA\cite{xla}, PyTorch JIT\cite{ptjit}, etc., which aim to speed up the inference of models. A bunch of optimization techniques have been incorporated into these compilers, such as constant folding, operator fusion, and tiling. It is highly possible for developers to make some mistakes when implementing these optimizations\cite{emp_tf}, so it is important to test deep learning compilers. But many optimization strategies, as listed before, cannot be triggered by a single tensor operator, which reveals the limitation of API-level fuzzing. While single-API invocations can easily be collected with approaches in FreeFuzz, it is non-trivial to get a number of diverse models consisting of multiple tensor operators which will serve as the essential part of oracles for testing deep learning compilers.

For graph-level fuzzers, usually, the more different models they can generate, the higher coverage they can achieve, and the more bugs they can find. There exist several model-level fuzzing works\cite{cradle, lemon, graphfuzzer, muffin, nnsmith}, and among them, NNSmith\cite{nnsmith} is the one which can generate the most diverse models. We only focus on three important components of it.

\begin{itemize}
    \item \textbf{Operator Specification} In order to generate models with theoretically arbitrary tensor shapes, NNSmith first defines a symbolic operator set. By "symbolic", it means each operator is modeled by two kinds of symbolic rules, where symbols denote the unknown tensor dimension sizes. 1) Shape transformation rules define how to compute output shapes given input shapes. 2) Shape constraints define the conditions that need to be satisfied by tensor shapes, such as each dimension size should be equal to or greater than zero.
    \item \textbf{Model Generation.} With symbolic defined operators, NNSmith first generates a symbolic graph/model operator by operator, where all tensor shapes are symbolic/uncertain. When inserting a new node/operator, it adds several symbolic constraints to a rule set, including the equivalence between input tensor shapes of this operator and output tensor shapes of its predecessors and the shape constraints in the operator's definition. After inserting a certain amount of nodes/operators, it invokes an SMT solver to assign concrete values to all symbols satisfying all accumulated symbolic rules, turning a symbolic graph into a concrete graph, also a valid input neural network for compilers.
    \item \textbf{Differential Testing.} Like FreeFuzz, NNSmith uses outputs given by the eager running mode as ground truth and compares the outputs produced by the graph/compiled mode with it, to reveal potential bugs of deep learning compilers. Occasionally, the deep learning framework cannot even execute the test case in eager mode, then it indicates a bug unrelated to the compiler but also needs attention.
\end{itemize}

Other similar works\cite{cradle, lemon, graphfuzzer, muffin} cannot generate models as diverse as NNSmith mainly because they are not capable of handling non-shape-preserving operators. It is trivial to build a deep learning model only with shape-preserving operators like element-wise operators (e.g. \texttt{relu}), but that is not the way developers build models for real-world applications. Therefore, a model generator needs to connect different operators with input/output tensors in different shapes while satisfying shape constraints. For example, the output shapes of an operator should be compatible with the input shapes of its successors, which poses challenges. In order to compose valid models with both shape-preserving operators and non-shape-preserving ones, works before NNSmith bypass the shape constraints issue by transforming non-shape-preserving operators to shape-preserving ones with extra layers, including Reshape and Linear, following them\cite{muffin}, or adding extra operators like padding/slicing to align shapes\cite{graphfuzzer}. However, this is still not how operators are connected in real-world models, and it also limits the model diversity.

NNSmith, as shown above, can generate very diverse models theoretically. However, in practice, modeling each tensor operator supported by deep learning libraries with handwritten symbolic rules needs tons of human effort. Thus, our work tries to automatically incorporate hundreds of diverse tensor operators to generate deep learning models for testing, which finally turns out to be very effective.


\bibliographystyle{unsrtnat}
\bibliography{ref/appendix}

\end{survey}
